{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, argparse, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import polars as pl \n",
    "\n",
    "import sklearn\n",
    "from   sklearn import metrics\n",
    "from   sklearn.metrics import RocCurveDisplay\n",
    "from   sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "from   sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from   sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import imblearn\n",
    "from   imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from   ax.service.ax_client import AxClient\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_np_from_pq(data):\n",
    "    # confirm no nulls\n",
    "    assert not data.null_count().pipe(sum).item() > 0\n",
    "    ys = data.select(pl.col('Label')).to_numpy().reshape(-1)\n",
    "\n",
    "    xs = data.drop(\n",
    "        pl.col('Header'),\n",
    "        pl.col('Label'), \n",
    "        pl.col('BasePair'), \n",
    "        pl.col('SeqLength'),\n",
    "        pl.col('Contig')\n",
    "        ).to_numpy()\n",
    "    \n",
    "    # convert logits to probability\n",
    "    xs = np.exp(xs)/(1+np.exp(xs))\n",
    "    return((ys, xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_y_onehot(ys, target_label):\n",
    "    y_onehot = np.zeros_like(ys)\n",
    "    y_onehot[ys == target_label] = 1\n",
    "    y_onehot = y_onehot.astype(int)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_metric(metrics_path, metric_dict):\n",
    "    # load and append to stats json if it exists\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            stats = json.load(f)\n",
    "    else:\n",
    "        stats = {}\n",
    "\n",
    "    stats |= metric_dict\n",
    "\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(json.dumps(stats, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 'Glycine_max'\n",
    "# Data options\n",
    "kmer = 1 \n",
    "BasePair = '3000'\n",
    "cv_fold = 0 \n",
    "cv_mode = 'tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ----\n",
    "## Prepare filters for the train/test (or train/validation) splits\n",
    "fold_df = pl.from_arrow(pq.read_table('./data/cv_folds.parquet'))\n",
    "\n",
    "match cv_mode:\n",
    "    case 'tuning':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_val\") & \n",
    "                    (pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_val\"))\n",
    "\n",
    "    case 'training':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_tst\"))\n",
    "\n",
    "## Load in data for target kmer\n",
    "# Basically this is a convoluted way to access parquet data as a np array\n",
    "# converting directly fails because we can't take a column as a pa.array and convert that. \n",
    "# get basepair lengths up to target bp\n",
    "bp = ['500', '1000', '3000', '5000', '10000', 'genome']\n",
    "bp = bp[0:(1 + [i for i in range(len(bp)) if bp[i] == BasePair][0])] # add one because we want to include the end of the slice\n",
    "\n",
    "df = pl.from_arrow(pq.read_table(f'./data/kmer{kmer}.parquet', \n",
    "                                 filters = [[('BasePair', 'in', bp)]])\n",
    "                                 ) \n",
    "\n",
    "# use inner join so that nans are not introduced for cases where a record is not present at all bp\n",
    "data_trn = fold_df.filter(mask_trn).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\")\n",
    "data_tst = fold_df.filter(mask_tst).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\") \n",
    "\n",
    "ys_trn, xs_trn = mk_np_from_pq(data = data_trn)\n",
    "ys_tst, xs_tst = mk_np_from_pq(data = data_tst)\n",
    "\n",
    "ys_trn = mk_y_onehot(ys = ys_trn, target_label = target_label)\n",
    "ys_tst = mk_y_onehot(ys = ys_tst, target_label = target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for exploring new models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vp",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
