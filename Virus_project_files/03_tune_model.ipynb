{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl \n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import sklearn\n",
    "import imblearn\n",
    "\n",
    "from   ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "# from ax.utils.notebook.plotting import init_notebook_plotting, render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_np_from_pq(data):\n",
    "    # confirm no nulls\n",
    "    assert not data.null_count().pipe(sum).item() > 0\n",
    "    ys = data.select(pl.col('Label')).to_numpy().reshape(-1)\n",
    "\n",
    "    xs = data.drop(\n",
    "        pl.col('Header'),\n",
    "        pl.col('Label'), \n",
    "        pl.col('BasePair'), \n",
    "        pl.col('SeqLength'),\n",
    "        pl.col('Contig')\n",
    "        ).to_numpy()\n",
    "    \n",
    "    # convert logits to probability\n",
    "    xs = np.exp(xs)/(1+np.exp(xs))\n",
    "    return((ys, xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_y_onehot(ys, target_label):\n",
    "    y_onehot = np.zeros_like(ys)\n",
    "    y_onehot[ys == target_label] = 1\n",
    "    y_onehot = y_onehot.astype(int)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values ----\n",
    "# (overwritten by parser)\n",
    "# Model options\n",
    "model_type = 'bknn'\n",
    "target_label = 'Glycine_max'\n",
    "# Data options\n",
    "kmer = 1 \n",
    "BasePair = '3000'\n",
    "cv_fold = 0 \n",
    "cv_mode = 'tuning'\n",
    "# Tuner options\n",
    "tuning_iterations = 20\n",
    "# Evaluation options\n",
    "k_job = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument parsing is not needed in a notebook\n",
    "\n",
    "# # argparse values ----\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # Model options\n",
    "# parser.add_argument(\"--model_type\",   type = str, help=\"model to be used: knn\")\n",
    "# parser.add_argument(\"--target_label\", type = str, help=\"prediction target: 'Drosophila_melanogaster',  'Glycine_max',  'Spodoptera_frugiperda', 'Vitis_vinifera', 'Zea_mays'\")\n",
    "# # Data options\n",
    "# parser.add_argument(\"--kmer\",     type = int, help=\"kmer data to be used: 1-6\")\n",
    "# parser.add_argument(\"--BasePair\", type = str, help=\"contig length: '500', '1000', '3000', '5000', '10000', 'genome' \")\n",
    "# parser.add_argument(\"--cv_fold\",  type = int, help=\"cv fold to be used: 0-4\")\n",
    "# parser.add_argument(\"--cv_mode\",  type = str, help=\"cv mode to be used: 'tuning' or 'training'\")\n",
    "# # Tuner options\n",
    "# parser.add_argument(\"--tuning_iterations\", type = int, help=\"number of trials to be run by the hyperparameter tuner.\")\n",
    "# # Evaluation options\n",
    "# parser.add_argument(\"--k_job\",             type = int, help=\"number of jobs for knn and bagging classifier\") \n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# if args.model_type:     model_type = args.model_type\n",
    "# if args.target_label: target_label = args.target_label\n",
    "# if args.kmer:                 kmer = args.kmer\n",
    "# if args.BasePair:         BasePair = args.BasePair\n",
    "# if args.cv_fold:           cv_fold = args.cv_fold\n",
    "# if args.cv_mode:           cv_mode = args.cv_mode\n",
    "# if args.tuning_iterations: tuning_iterations = args.tuning_iterations\n",
    "# if args.k_job:               k_job = args.k_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This is where new models should be added. \n",
    "# New model_types should have at minimum:\n",
    "# 1. a search space (`exp_search_space`)\n",
    "# 2. an evaluation function for Ax. (`evaluate`)\n",
    "# NOTE many options are provided in the script verison of this file.\n",
    "\n",
    "match model_type:\n",
    "    case 'bknn':        \n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "        exp_search_space = [\n",
    "            {\n",
    "                \"name\": \"weights\",\n",
    "                \"type\": \"choice\",\n",
    "                \"values\": ['uniform', 'distance'], \n",
    "                \"is_ordered\": True,\n",
    "                \"sort_values\": False,\n",
    "                \"value_type\": \"str\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"k\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [1, 50],\n",
    "                \"value_type\": \"int\", \n",
    "                \"log_scale\": False,  \n",
    "            },\n",
    "            {\n",
    "                \"name\": \"n_estimators\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [1, 10],\n",
    "                \"value_type\": \"int\", \n",
    "                \"log_scale\": False,  \n",
    "            }\n",
    "        ]\n",
    "        # define evaluation function for Ax to use. \n",
    "        # NOTE: this is written following the approach used by _Zhang et al. 2019_ of using a random seed to set up a test/train split.\n",
    "        # I (Daniel) think it would be better to use a test/validation/training split and make definition of these _explicit_ even if `train_test_split` is generating these.  \n",
    "        def evaluate(\n",
    "                y_train, X_train, \n",
    "                y_test,  X_test,\n",
    "                parameterization = {'weights': 'uniform', 'k': 6}, \n",
    "                k_job = 10\n",
    "                ):\n",
    "            knn = KNeighborsClassifier(\n",
    "                n_neighbors=parameterization['k'],\n",
    "                weights = parameterization['weights'], \n",
    "                n_jobs= k_job)\n",
    "\n",
    "            model = BalancedBaggingClassifier(\n",
    "                estimator=knn, \n",
    "                n_estimators =parameterization['n_estimators'], \n",
    "                n_jobs = k_job)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            return {'score': ( model.score(X_test, y_test) )}\n",
    "    \n",
    "    case _:\n",
    "        print(f\"Model {model_type} is not defined!\")\n",
    "        assert True == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ----\n",
    "## Prepare filters for the train/test (or train/validation) splits\n",
    "fold_df = pl.from_arrow(pq.read_table('./data/cv_folds.parquet'))\n",
    "\n",
    "match cv_mode:\n",
    "    case 'tuning':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_val\") & \n",
    "                    (pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_val\"))\n",
    "\n",
    "    case 'training':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_tst\"))\n",
    "\n",
    "## Load in data for target kmer\n",
    "# Basically this is a convoluted way to access parquet data as a np array\n",
    "# converting directly fails because we can't take a column as a pa.array and convert that. \n",
    "# get basepair lengths up to target bp\n",
    "bp = ['500', '1000', '3000', '5000', '10000', 'genome']\n",
    "bp = bp[0:(1 + [i for i in range(len(bp)) if bp[i] == BasePair][0])] # add one because we want to include the end of the slice\n",
    "\n",
    "df = pl.from_arrow(pq.read_table(f'./data/kmer{kmer}.parquet', \n",
    "                                 filters = [[('BasePair', 'in', bp)]])\n",
    "                                 ) \n",
    "\n",
    "# use inner join so that nans are not introduced for cases where a record is not present at all bp\n",
    "data_trn = fold_df.filter(mask_trn).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\")\n",
    "data_tst = fold_df.filter(mask_tst).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\") \n",
    "\n",
    "ys_trn, xs_trn = mk_np_from_pq(data = data_trn)\n",
    "ys_tst, xs_tst = mk_np_from_pq(data = data_tst)\n",
    "\n",
    "ys_trn = mk_y_onehot(ys = ys_trn, target_label = target_label)\n",
    "ys_tst = mk_y_onehot(ys = ys_tst, target_label = target_label)\n",
    "\n",
    "# Restore or create ax tuner\n",
    "exp_name  = f\"{model_type}-{target_label}-kmer{kmer}-bp{BasePair}-fold{cv_fold}\"\n",
    "json_path = f\"./models/tune/{exp_name}.json\"\n",
    "if not os.path.exists(json_path):\n",
    "    ax_client = AxClient()\n",
    "    ax_client.create_experiment(\n",
    "        name=exp_name,\n",
    "        parameters=exp_search_space,\n",
    "        objectives={\"score\": ObjectiveProperties(minimize=False)} # Score is mean accuracy so we want to maximize it\n",
    "    )\n",
    "else: \n",
    "    ax_client = (AxClient.load_from_json_file(filepath = json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The exception below is also present in the example documents for Ax. \n",
    "# Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
    "for i in range(tuning_iterations):\n",
    "    params, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(\n",
    "        y_train= ys_trn, X_train= xs_trn, \n",
    "        y_test= ys_tst, X_test= xs_tst,\n",
    "        parameterization = params, \n",
    "        k_job = k_job\n",
    "        ))\n",
    "\n",
    "ax_client.save_to_json_file(filepath = json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if working interactively in a jupyter notebook here are some useful methods. \n",
    "# ax_client.get_trials_data_frame()\n",
    "# best_parameters, values = ax_client.get_best_parameters()\n",
    "#\n",
    "#  # This should be in its own cell\n",
    "# init_notebook_plotting()\n",
    "#\n",
    "# # next cell\n",
    "# render(ax_client.get_optimization_trace(objective_optimum=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
