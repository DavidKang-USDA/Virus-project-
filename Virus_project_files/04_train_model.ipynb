{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, argparse, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import polars as pl \n",
    "\n",
    "import sklearn\n",
    "from   sklearn import metrics\n",
    "from   sklearn.metrics import RocCurveDisplay\n",
    "from   sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "from   sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from   sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import imblearn\n",
    "from   imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from   ax.service.ax_client import AxClient\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_np_from_pq(data):\n",
    "    # confirm no nulls\n",
    "    assert not data.null_count().pipe(sum).item() > 0\n",
    "    ys = data.select(pl.col('Label')).to_numpy().reshape(-1)\n",
    "\n",
    "    xs = data.drop(\n",
    "        pl.col('Header'),\n",
    "        pl.col('Label'), \n",
    "        pl.col('BasePair'), \n",
    "        pl.col('SeqLength'),\n",
    "        pl.col('Contig')\n",
    "        ).to_numpy()\n",
    "    \n",
    "    # convert logits to probability\n",
    "    xs = np.exp(xs)/(1+np.exp(xs))\n",
    "    return((ys, xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_y_onehot(ys, target_label):\n",
    "    y_onehot = np.zeros_like(ys)\n",
    "    y_onehot[ys == target_label] = 1\n",
    "    y_onehot = y_onehot.astype(int)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_metric(metrics_path, metric_dict):\n",
    "    # load and append to stats json if it exists\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            stats = json.load(f)\n",
    "    else:\n",
    "        stats = {}\n",
    "\n",
    "    stats |= metric_dict\n",
    "\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(json.dumps(stats, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_plts(y, yhat, plt_path):\n",
    "    cm = confusion_matrix(y, yhat)\n",
    "    cm_display = ConfusionMatrixDisplay(cm)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, yhat, pos_label=1)\n",
    "    prec, recall, _ = precision_recall_curve(y, yhat, pos_label=1)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "    pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)\n",
    "\n",
    "    # return (cm_display, roc_display, pr_display)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 8))\n",
    "    roc_display.plot(ax=ax1)\n",
    "    pr_display.plot(ax=ax2)\n",
    "    cm_display.plot(ax=ax3)\n",
    "    plt.savefig(plt_path)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mk_plts(y, yhat, plt_path):\n",
    "    cm = confusion_matrix(y, yhat)\n",
    "    cm_display = ConfusionMatrixDisplay(cm)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, yhat, pos_label=1)\n",
    "    prec, recall, _ = precision_recall_curve(y, yhat, pos_label=1)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "    pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)\n",
    "\n",
    "    # return (cm_display, roc_display, pr_display)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 8))\n",
    "    roc_display.plot(ax=ax1)\n",
    "    pr_display.plot(ax=ax2)\n",
    "    cm_display.plot(ax=ax3)\n",
    "    if plt_path != None:\n",
    "        plt.savefig(plt_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default values ----\n",
    "# (overwritten by parser)\n",
    "# Model options\n",
    "model_type = 'bknn'\n",
    "target_label = 'Glycine_max'\n",
    "# Data options\n",
    "kmer = 1 \n",
    "BasePair = '3000'\n",
    "cv_fold = 0 \n",
    "cv_mode = 'tuning'\n",
    "# Tuner options\n",
    "tuning_iterations = 20\n",
    "# Evaluation options\n",
    "k_job = 10\n",
    "\n",
    "# exp_json = './example.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument parsing is not needed in a notebook\n",
    "\n",
    "# # argparse values ----\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # Model options\n",
    "# parser.add_argument(\"--model_type\",   type = str, help=\"model to be used: knn\")\n",
    "# parser.add_argument(\"--target_label\", type = str, help=\"prediction target: 'Drosophila_melanogaster',  'Glycine_max',  'Spodoptera_frugiperda', 'Vitis_vinifera', 'Zea_mays'\")\n",
    "# # Data options\n",
    "# parser.add_argument(\"--kmer\",     type = int, help=\"kmer data to be used: 1-6\")\n",
    "# parser.add_argument(\"--BasePair\", type = str, help=\"contig length: '500', '1000', '3000', '5000', '10000', 'genome' \")\n",
    "# parser.add_argument(\"--cv_fold\",  type = int, help=\"cv fold to be used: 0-4\")\n",
    "# parser.add_argument(\"--cv_mode\",  type = str, help=\"cv mode to be used: 'tuning' or 'training'\")\n",
    "# # Tuner options\n",
    "# parser.add_argument(\"--tuning_iterations\", type = int, help=\"number of trials to be run by the hyperparameter tuner.\")\n",
    "# # Evaluation options\n",
    "# parser.add_argument(\"--k_job\",             type = int, help=\"number of jobs for knn and bagging classifier\") \n",
    "# # Optional options\n",
    "# parser.add_argument(\"--exp_json\",  type = str, help=\"optional. path to json file containing hyperparameter specifications. If provided the name will be appended to the experiment output.\")\n",
    "# parser.add_argument(\"--exp_name_append\",  type = str, help=\"optional. ignored unless --exp_json is passed as well. appends string to experiment name instead of appending exp_json\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# if args.model_type:     model_type = args.model_type\n",
    "# if args.target_label: target_label = args.target_label\n",
    "# if args.kmer:                 kmer = args.kmer\n",
    "# if args.BasePair:         BasePair = args.BasePair\n",
    "# if args.cv_fold:           cv_fold = args.cv_fold\n",
    "# if args.cv_mode:           cv_mode = args.cv_mode\n",
    "# if args.tuning_iterations: tuning_iterations = args.tuning_iterations\n",
    "# if args.k_job:               k_job = args.k_job\n",
    "# if args.exp_json:         exp_json = args.exp_json\n",
    "# if args.exp_name_append:  exp_name_append = args.exp_name_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NOTE: This is where new models should be added. \n",
    "# New model_types should have at minimum:\n",
    "# 1. a search space (`exp_search_space`)\n",
    "# 2. an evaluation function for Ax. (`evaluate`)\n",
    "# NOTE many options are provided in the script verison of this file.\n",
    "\n",
    "match model_type:       \n",
    "    case 'bknn':        \n",
    "        # slight modification of `evaluate()`\n",
    "        def train_model(\n",
    "                y_train, X_train,\n",
    "                parameterization = {'weights': 'uniform', 'k': 6}, \n",
    "                k_job = 10, \n",
    "                **kwargs\n",
    "                ):\n",
    "            knn = KNeighborsClassifier(\n",
    "                n_neighbors=parameterization['k'],\n",
    "                weights = parameterization['weights'], \n",
    "                n_jobs= k_job)\n",
    "\n",
    "            model = BalancedBaggingClassifier(\n",
    "                estimator=knn, \n",
    "                n_estimators =parameterization['n_estimators'], \n",
    "                n_jobs = k_job)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            return model\n",
    "        \n",
    "    case _:\n",
    "        print(f\"Model {model_type} is not defined!\")\n",
    "        assert True == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep ----\n",
    "## Prepare filters for the train/test (or train/validation) splits\n",
    "fold_df = pl.from_arrow(pq.read_table('./data/cv_folds.parquet'))\n",
    "\n",
    "match cv_mode:\n",
    "    case 'tuning':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_val\") & \n",
    "                    (pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_val\"))\n",
    "\n",
    "    case 'training':\n",
    "        mask_trn = ((pl.col('Fold') != f\"fold_{cv_fold}_tst\"))\n",
    "        \n",
    "        mask_tst = ((pl.col('Fold') == f\"fold_{cv_fold}_tst\"))\n",
    "\n",
    "## Load in data for target kmer\n",
    "# Basically this is a convoluted way to access parquet data as a np array\n",
    "# converting directly fails because we can't take a column as a pa.array and convert that. \n",
    "# get basepair lengths up to target bp\n",
    "bp = ['500', '1000', '3000', '5000', '10000', 'genome']\n",
    "bp = bp[0:(1 + [i for i in range(len(bp)) if bp[i] == BasePair][0])] # add one because we want to include the end of the slice\n",
    "\n",
    "df = pl.from_arrow(pq.read_table(f'./data/kmer{kmer}.parquet', \n",
    "                                 filters = [[('BasePair', 'in', bp)]])\n",
    "                                 ) \n",
    "\n",
    "# use inner join so that nans are not introduced for cases where a record is not present at all bp\n",
    "data_trn = fold_df.filter(mask_trn).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\")\n",
    "data_tst = fold_df.filter(mask_tst).drop(pl.col('Fold')).join(df, on=['Header', 'Label'], how=\"inner\") \n",
    "\n",
    "ys_trn, xs_trn = mk_np_from_pq(data = data_trn)\n",
    "ys_tst, xs_tst = mk_np_from_pq(data = data_tst)\n",
    "\n",
    "ys_trn = mk_y_onehot(ys = ys_trn, target_label = target_label)\n",
    "ys_tst = mk_y_onehot(ys = ys_tst, target_label = target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for arbitrary model fitting and and the addition of ad hoc text to the exp_name (for saving)\n",
    "# Example of writing json from python. Expected behavior is to write from python (interactive) or manually write these files.\n",
    "# with open('./example.json', 'w') as f:\n",
    "#     f.write(json.dumps({\"weights\": \"uniform\", \"k\": 1, \"n_estimators\": 10}, indent=4, sort_keys=True))\n",
    "\n",
    "# if `exp_json` exists in the global scope, load the parameters in it.\n",
    "if 'exp_json' in globals():\n",
    "    with open(exp_json, 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    if 'exp_name_append' in globals():\n",
    "        # if `exp_name_append` was provided append this nickname to the save name.\n",
    "        exp_name  = f\"{model_type}-{target_label}-kmer{kmer}-bp{BasePair}-fold{cv_fold}-{exp_name_append}\"\n",
    "    else:\n",
    "        # otherwise we'll use `exp_json` to keep everthing clear and organized.\n",
    "        exp_name  = f\"{model_type}-{target_label}-kmer{kmer}-bp{BasePair}-fold{cv_fold}-{exp_json.split('/')[-1]}\"\n",
    "else:\n",
    "    # Restore ax tuner and get the best documented entry\n",
    "    exp_name  = f\"{model_type}-{target_label}-kmer{kmer}-bp{BasePair}-fold{cv_fold}\"\n",
    "    json_path = f\"./models/tune/{exp_name}.json\"\n",
    "    ax_client = (AxClient.load_from_json_file(filepath = json_path))\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "    params = best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model ----\n",
    "model = train_model(\n",
    "    y_train= ys_trn, X_train= xs_trn, \n",
    "    parameterization = params, \n",
    "    k_job = k_job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "pickle_path = f\"./models/prod/{exp_name}.pkl\"\n",
    "\n",
    "# There are more secure ways to save sklearn models than pickle. \n",
    "# This is okay for our purposes. \n",
    "# In the future migration may make sense.\n",
    "# with open(pickle_path, 'wb') as f:\n",
    "#     pickle.dump(model, f, protocol=5)\n",
    "\n",
    "# to read a pickled model:\n",
    "# with open(pickle_path, 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models ----\n",
    "yhat_trn = model.predict(xs_trn)\n",
    "yhat_tst = model.predict(xs_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics in json dictionary\n",
    "# prep for auc\n",
    "fpr_trn, tpr_trn, thresholds_trn = sklearn.metrics.roc_curve(ys_trn, yhat_trn, pos_label=1)\n",
    "fpr_tst, tpr_tst, thresholds_tst = sklearn.metrics.roc_curve(ys_tst, yhat_tst, pos_label=1)\n",
    "\n",
    "append_metric(\n",
    "    metrics_path = f\"./models/prod/{exp_name}__metrics.json\", \n",
    "    metric_dict = {\n",
    "        # Accuracy\n",
    "        'accuracy_trn': sklearn.metrics.accuracy_score(y_true= ys_trn, y_pred= yhat_trn),\n",
    "        'accuracy_tst': sklearn.metrics.accuracy_score(y_true= ys_tst, y_pred= yhat_tst),\n",
    "        # Recall rate\n",
    "        'recall_trn': sklearn.metrics.recall_score(y_true= ys_trn, y_pred= yhat_trn, average='binary'),\n",
    "        'recall_tst': sklearn.metrics.recall_score(y_true= ys_tst, y_pred= yhat_tst, average='binary'),\n",
    "\n",
    "        # Specificity\n",
    "        'specificity_trn': sklearn.metrics.recall_score(y_true= ys_trn, y_pred= yhat_trn, average='binary', pos_label=0),\n",
    "        'specificity_tst': sklearn.metrics.recall_score(y_true= ys_tst, y_pred= yhat_tst, average='binary', pos_label=0),\n",
    "\n",
    "        # Precision\n",
    "        'precision_trn': sklearn.metrics.precision_score(y_true= ys_trn, y_pred= yhat_trn, average='binary'),\n",
    "        'precision_tst': sklearn.metrics.precision_score(y_true= ys_tst, y_pred= yhat_tst, average='binary'),\n",
    "\n",
    "        # AUC\n",
    "        'auc_trn': metrics.auc(fpr_trn, tpr_trn),\n",
    "        'auc_tst': metrics.auc(fpr_tst, tpr_tst)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Write out predictions with probabilites and convenience column for the test/train split\n",
    "y_proba_trn = model.predict_proba(xs_trn)\n",
    "yhat_df_trn =(\n",
    "    data_trn\n",
    "    .select(pl.col('Header'), pl.col('Label'))\n",
    "    .with_columns( pl.Series(name=\"Yhat\",    values=yhat_trn,      dtype=pl.Int16))\n",
    "    .with_columns( pl.Series(name=\"ProbPos\", values=y_proba_trn[:, 1], dtype=pl.Float64))\n",
    "    .with_columns(pl.lit(False).alias(\"TestSet\"))\n",
    ")\n",
    "\n",
    "\n",
    "y_proba_tst = model.predict_proba(xs_tst)\n",
    "yhat_df_tst =(\n",
    "    data_tst\n",
    "    .select(pl.col('Header'), pl.col('Label'))\n",
    "    .with_columns( pl.Series(name=\"Yhat\",    values=yhat_tst,      dtype=pl.Int16))\n",
    "    .with_columns( pl.Series(name=\"ProbPos\", values=y_proba_tst[:, 1], dtype=pl.Float64))\n",
    "    .with_columns(pl.lit(True).alias(\"TestSet\"))\n",
    ")\n",
    "\n",
    "# pl.concat([yhat_df_trn, yhat_df_tst]).write_parquet(f\"./models/prod/{exp_name}__predictions.parquet\")\n",
    "\n",
    "# # Write out visualizations\n",
    "# mk_plts(y = ys_trn, yhat = yhat_trn, plt_path = f\"./models/prod/{exp_name}__trn.png\")\n",
    "# mk_plts(y = ys_tst, yhat = yhat_tst, plt_path = f\"./models/prod/{exp_name}__tst.png\")\n",
    "\n",
    "# if plt_path is not provided then the plot should be visualized not saved.\n",
    "mk_plts(y = ys_trn, yhat = yhat_trn)\n",
    "mk_plts(y = ys_tst, yhat = yhat_tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vp",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
